# 🎶 Coding Through Music: Sensor-Driven MIDI Instruments

Welcome to a project designed to blend **coding**, **music**, and **sensor technology**—creating a hands-on, interactive learning experience where students train sensors to play musical notes using algorithmic thinking.

---

## 📌 Project Goals

- **Introduce Coding Through Music:** Use the MIDI protocol to make coding engaging by turning code into sound.
- **Foster Algorithmic Thinking:** Empower students to build custom instruments by designing logic that maps sensor inputs to musical outputs.
- **Bridge Disciplines:** Explore the intersections between **engineering** and **music education**, highlighting shared principles like pattern recognition, timing, and signal processing.

---

## 🌐 Overview of Sites

1. **Training Page**  
   Train musical notes by associating specific sensor input values with MIDI notes using a K-Nearest Neighbor algorithm.

2. **Visualizer Page**  
   Display sensor input data in real time, showing both the numeric values and their corresponding musical note representations on a staff.

3. **Performance Page**  
   Send live sensor data and hear the corresponding MIDI notes play back immediately—transforming movement or pressure into sound.

---

## ✨ Features

### 🔌 Supported Technology

- **LEGO/Hub Science Kit Sensors** (force, light, distance, etc.)
- **MIDI Output Support** for real-time note playback
- **PyScript + Web Channels** for seamless sensor-to-sound communication

### 🧠 Supported Algorithms

- **K-Nearest Neighbor (KNN)**  
  A lightweight, interpretable machine learning algorithm used for classifying incoming sensor readings as musical notes based on trained data.

---

This project is ideal for classrooms or workshops seeking to combine **STEM** and **arts-based learning** in a meaningful, interactive way.

